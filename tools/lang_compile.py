#!/usr/bin/env python3
"""
Translation hash table generator for CrossPoint Reader i18n.

Reads en.lang reference file, assigns stable IDs to each translation key,
computes FNV-1a hashes, checks for collisions, and generates a flash-resident
hash table (LangKeys.inc) for O(1) amortized lookup at runtime.

Usage:
    python tools/lang_compile.py                  # generate LangKeys.inc
    python tools/lang_compile.py --validate       # also validate all .lang files
    python tools/lang_compile.py --force          # regenerate even if unchanged

Can also be used as a PlatformIO pre-build script (extra_scripts = pre:tools/lang_compile.py).
"""

import os
import sys
import hashlib
import argparse

# ---------------------------------------------------------------------------
# Paths (resolved lazily to support both standalone and PlatformIO execution)
# ---------------------------------------------------------------------------

def _get_project_dir():
    """Determine the project root directory."""
    # When run as PlatformIO extra_script, __file__ is not defined.
    # Try the PlatformIO env first, then fall back to __file__.
    try:
        Import("env")
        return env.subst("$PROJECT_DIR")
    except Exception:
        pass
    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

PROJECT_DIR = _get_project_dir()
EN_LANG_PATH = os.path.join(PROJECT_DIR, "SD_card_root", "config", "lang", "en.lang")
LANG_DIR = os.path.join(PROJECT_DIR, "SD_card_root", "config", "lang")
OUTPUT_PATH = os.path.join(PROJECT_DIR, "src", "i18n", "LangKeys.inc")
STAMP_PATH = os.path.join(PROJECT_DIR, ".lang_compile_stamp")


# ---------------------------------------------------------------------------
# FNV-1a 32-bit (must match the C++ implementation exactly)
# ---------------------------------------------------------------------------

def fnv1a_32(s: str) -> int:
    """Compute FNV-1a 32-bit hash of a UTF-8 string."""
    h = 2166136261
    for byte in s.encode("utf-8"):
        h = ((h ^ byte) * 16777619) & 0xFFFFFFFF
    return h


# ---------------------------------------------------------------------------
# .lang file parser
# ---------------------------------------------------------------------------

def parse_lang_file(path: str) -> dict:
    """Parse a .lang file and return {key: value} dict."""
    entries = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            eq = line.find("=")
            if eq < 0:
                continue
            key = line[:eq].strip()
            value = line[eq + 1:]  # don't strip leading spaces from value
            if key.startswith("language."):
                continue
            if key:
                entries[key] = value
    return entries


# ---------------------------------------------------------------------------
# Hash table generation
# ---------------------------------------------------------------------------

def next_power_of_2(n: int) -> int:
    """Return the smallest power of 2 >= n."""
    p = 1
    while p < n:
        p <<= 1
    return p


def build_hash_table(keys_with_ids: list) -> tuple:
    """
    Build an open-addressing hash table with linear probing.

    Args:
        keys_with_ids: list of (key_string, id, hash) tuples

    Returns:
        (table_size, table) where table is a list of (hash, id) tuples.
        Empty slots have hash=0.
    """
    # Table size: ~2x the number of keys for ~50% load factor
    table_size = next_power_of_2(len(keys_with_ids) * 2)
    table = [(0, 0)] * table_size  # (hash, id)

    for key, kid, h in keys_with_ids:
        slot = h % table_size
        while table[slot][0] != 0:
            slot = (slot + 1) % table_size
        table[slot] = (h, kid)

    return table_size, table


def generate_inc(keys: list, table_size: int, table: list, output_path: str):
    """Generate the LangKeys.inc C++ header file."""
    lines = []
    lines.append("// Auto-generated by tools/lang_compile.py from en.lang")
    lines.append("// DO NOT EDIT — regenerate with: python tools/lang_compile.py")
    lines.append("//")
    lines.append(f"// Keys: {len(keys)}, Table size: {table_size}, "
                 f"Load factor: {len(keys) / table_size:.1%}")
    lines.append("#pragma once")
    lines.append("#include <cstdint>")
    lines.append("")
    lines.append(f"static constexpr uint16_t LANG_KEY_COUNT = {len(keys)};")
    lines.append(f"static constexpr uint16_t LANG_HASH_TABLE_SIZE = {table_size};")
    lines.append("")
    lines.append("struct LangHashEntry {")
    lines.append("  uint32_t hash;  // FNV-1a 32-bit hash of key, 0 = empty slot")
    lines.append("  uint16_t id;    // index into values array")
    lines.append("};")
    lines.append("")
    lines.append(f"static const LangHashEntry LANG_HASH_TABLE[{table_size}] = {{")

    # Build a reverse map: id → key name (for comments)
    id_to_key = {kid: key for key, kid, _ in keys}

    for i, (h, kid) in enumerate(table):
        if h == 0:
            lines.append(f"    {{0, 0}},")
        else:
            key_name = id_to_key.get(kid, "?")
            # Escape any problematic chars for C++ comment
            safe_name = key_name.replace("*/", "* /")
            lines.append(f"    {{0x{h:08X}, {kid}}},  /* {safe_name} */")

    lines.append("};")
    lines.append("")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")


# ---------------------------------------------------------------------------
# Validation
# ---------------------------------------------------------------------------

def validate_lang_files(reference_keys: set, lang_dir: str):
    """Validate all .lang files against the reference key set."""
    import glob
    lang_files = sorted(glob.glob(os.path.join(lang_dir, "*.lang")))

    for path in lang_files:
        filename = os.path.basename(path)
        if filename == "en.lang":
            continue

        entries = parse_lang_file(path)
        lang_keys = set(entries.keys())

        missing = reference_keys - lang_keys
        extra = lang_keys - reference_keys
        translated = len(reference_keys & lang_keys)
        total = len(reference_keys)

        print(f"  {filename}: {translated}/{total} keys translated")
        if missing:
            print(f"    WARNING: {len(missing)} missing keys (will fall back to English):")
            for k in sorted(missing)[:5]:
                print(f"      - {k}")
            if len(missing) > 5:
                print(f"      ... and {len(missing) - 5} more")
        if extra:
            print(f"    WARNING: {len(extra)} unknown keys (will be ignored):")
            for k in sorted(extra)[:5]:
                print(f"      - {k}")
            if len(extra) > 5:
                print(f"      ... and {len(extra) - 5} more")


# ---------------------------------------------------------------------------
# Staleness check
# ---------------------------------------------------------------------------

def is_up_to_date(en_lang_path: str, output_path: str, stamp_path: str) -> bool:
    """Check if LangKeys.inc is up to date with en.lang."""
    if not os.path.exists(output_path) or not os.path.exists(stamp_path):
        return False

    with open(en_lang_path, "rb") as f:
        current_hash = hashlib.md5(f.read()).hexdigest()

    try:
        with open(stamp_path, "r") as f:
            saved_hash = f.read().strip()
    except (IOError, OSError):
        return False

    return current_hash == saved_hash


def write_stamp(en_lang_path: str, stamp_path: str):
    """Write the MD5 stamp of en.lang for staleness checking."""
    with open(en_lang_path, "rb") as f:
        current_hash = hashlib.md5(f.read()).hexdigest()
    with open(stamp_path, "w") as f:
        f.write(current_hash)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main(force: bool = False, validate: bool = False):
    print(f"[lang_compile] Reading: {os.path.relpath(EN_LANG_PATH, PROJECT_DIR)}")

    if not os.path.exists(EN_LANG_PATH):
        print(f"[lang_compile] ERROR: {EN_LANG_PATH} not found")
        sys.exit(1)

    # Staleness check
    if not force and is_up_to_date(EN_LANG_PATH, OUTPUT_PATH, STAMP_PATH):
        print("[lang_compile] LangKeys.inc is up to date, skipping")
        return

    # 1. Parse en.lang
    entries = parse_lang_file(EN_LANG_PATH)
    if not entries:
        print("[lang_compile] ERROR: No translation keys found in en.lang")
        sys.exit(1)

    # 2. Sort keys alphabetically → stable IDs
    sorted_keys = sorted(entries.keys())
    print(f"[lang_compile] Found {len(sorted_keys)} translation keys")

    # 3. Compute hashes and check for collisions
    keys_with_ids = []
    hash_to_key = {}
    collisions = []

    for kid, key in enumerate(sorted_keys):
        h = fnv1a_32(key)

        # Check hash != 0 (empty slot marker)
        if h == 0:
            print(f"[lang_compile] ERROR: Key '{key}' has hash 0 (reserved for empty slots)")
            print(f"  Please modify this key slightly (e.g., add a trailing space)")
            sys.exit(1)

        # Check for hash collisions between different keys
        if h in hash_to_key:
            collisions.append((key, hash_to_key[h], h))
        else:
            hash_to_key[h] = key

        keys_with_ids.append((key, kid, h))

    if collisions:
        print(f"[lang_compile] ERROR: {len(collisions)} hash collision(s) detected!")
        for key1, key2, h in collisions:
            print(f"  '{key1}' and '{key2}' both hash to 0x{h:08X}")
            print(f"  Please modify one of these keys slightly")
        sys.exit(1)

    print(f"[lang_compile] No hash collisions detected")

    # 4. Build hash table
    table_size, table = build_hash_table(keys_with_ids)
    load_factor = len(keys_with_ids) / table_size
    print(f"[lang_compile] Hash table: {table_size} slots, "
          f"load factor {load_factor:.1%}")

    # 5. Generate LangKeys.inc
    generate_inc(keys_with_ids, table_size, table, OUTPUT_PATH)
    write_stamp(EN_LANG_PATH, STAMP_PATH)
    print(f"[lang_compile] Generated: {os.path.relpath(OUTPUT_PATH, PROJECT_DIR)}")

    # 6. Validate other .lang files
    if validate:
        print(f"[lang_compile] Validating translations:")
        reference_keys = set(sorted_keys)
        validate_lang_files(reference_keys, LANG_DIR)

    print(f"[lang_compile] Done")


# ---------------------------------------------------------------------------
# Entry point
# ---------------------------------------------------------------------------

def _is_platformio():
    """Check if we're running inside PlatformIO's SCons environment."""
    try:
        Import("env")
        return True
    except Exception:
        return False

if _is_platformio():
    # Running as PlatformIO pre-build script
    main(force=False, validate=False)
elif __name__ == "__main__":
    # Running standalone
    parser = argparse.ArgumentParser(description="Generate i18n hash table")
    parser.add_argument("--force", action="store_true",
                        help="Regenerate even if en.lang hasn't changed")
    parser.add_argument("--validate", action="store_true",
                        help="Validate all .lang files against en.lang")
    args = parser.parse_args()
    main(force=args.force, validate=args.validate)
